import{_ as a,c as s,o as t,ae as i}from"./chunks/framework.DCsj0v3X.js";const u=JSON.parse('{"title":"How It Works","description":"","frontmatter":{},"headers":[],"relativePath":"how-it-works.md","filePath":"how-it-works.md"}'),n={name:"how-it-works.md"};function o(r,e,l,h,p,c){return t(),s("div",null,[...e[0]||(e[0]=[i(`<h1 id="how-it-works" tabindex="-1">How It Works <a class="header-anchor" href="#how-it-works" aria-label="Permalink to &quot;How It Works&quot;">​</a></h1><p>This page explains the internal architecture and design decisions behind React Voice Action Router.</p><h2 id="overview" tabindex="-1">Overview <a class="header-anchor" href="#overview" aria-label="Permalink to &quot;Overview&quot;">​</a></h2><p>React Voice Action Router is built on three core concepts:</p><ol><li>A centralized command registry</li><li>A two-phase matching algorithm</li><li>An adapter pattern for AI providers</li></ol><h2 id="the-command-registry" tabindex="-1">The Command Registry <a class="header-anchor" href="#the-command-registry" aria-label="Permalink to &quot;The Command Registry&quot;">​</a></h2><p>At the heart of the library is a command registry managed by the VoiceControlProvider. This registry is a Map data structure that stores all currently registered voice commands.</p><h3 id="how-registration-works" tabindex="-1">How Registration Works <a class="header-anchor" href="#how-registration-works" aria-label="Permalink to &quot;How Registration Works&quot;">​</a></h3><p>When you use the useVoiceCommand hook, the following happens:</p><ol><li>The hook calls the register function from the VoiceContext</li><li>The command is added to the internal Map using its ID as the key</li><li>The state is updated to reflect the new list of active commands</li><li>When the component unmounts, the cleanup function calls unregister to remove the command</li></ol><p>This design ensures:</p><ul><li>Commands are automatically cleaned up when components unmount</li><li>There are no duplicate command IDs (later registrations overwrite earlier ones)</li><li>Command lookups are fast with O(1) time complexity</li></ul><h3 id="the-proxy-pattern" tabindex="-1">The Proxy Pattern <a class="header-anchor" href="#the-proxy-pattern" aria-label="Permalink to &quot;The Proxy Pattern&quot;">​</a></h3><p>The useVoiceCommand hook uses a clever proxy pattern to handle action updates:</p><div class="language-tsx vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">tsx</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">// The hook keeps a ref to the latest command</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">const</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> commandRef</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> useRef</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(command);</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">// The registered command calls through the ref</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">const</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> proxyCommand</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> {</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  id: command.id,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  description: command.description,</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">  action</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: () </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> commandRef.current.</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">action</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(),</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">};</span></span></code></pre></div><p>This means if your action function changes due to state updates, you do not need to re-register the command. The proxy always calls the latest version of the action.</p><h2 id="two-phase-matching" tabindex="-1">Two-Phase Matching <a class="header-anchor" href="#two-phase-matching" aria-label="Permalink to &quot;Two-Phase Matching&quot;">​</a></h2><p>When a voice transcript is processed, the router uses a two-phase matching algorithm:</p><h3 id="phase-1-exact-match" tabindex="-1">Phase 1: Exact Match <a class="header-anchor" href="#phase-1-exact-match" aria-label="Permalink to &quot;Phase 1: Exact Match&quot;">​</a></h3><p>The first phase looks for an exact match between the transcript and registered phrases:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>User says: &quot;go home&quot;</span></span>
<span class="line"><span>Registered phrases: [&quot;go home&quot;, &quot;open settings&quot;, &quot;toggle theme&quot;]</span></span>
<span class="line"><span>Result: Exact match found for &quot;go home&quot; -&gt; Command executes immediately</span></span></code></pre></div><p>This phase has zero latency because it does not require any API calls. If your users know the exact phrases, they get instant response.</p><h3 id="phase-2-ai-fuzzy-match" tabindex="-1">Phase 2: AI Fuzzy Match <a class="header-anchor" href="#phase-2-ai-fuzzy-match" aria-label="Permalink to &quot;Phase 2: AI Fuzzy Match&quot;">​</a></h3><p>If no exact match is found, the transcript is sent to the AI adapter:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>User says: &quot;take me to the homepage&quot;</span></span>
<span class="line"><span>No exact match found</span></span>
<span class="line"><span>AI analyzes: &quot;take me to the homepage&quot; against all command descriptions</span></span>
<span class="line"><span>AI returns: { commandId: &quot;go_home&quot; }</span></span>
<span class="line"><span>Command executes</span></span></code></pre></div><p>This phase handles natural language variations and understands intent even when the exact words differ.</p><h2 id="the-adapter-pattern" tabindex="-1">The Adapter Pattern <a class="header-anchor" href="#the-adapter-pattern" aria-label="Permalink to &quot;The Adapter Pattern&quot;">​</a></h2><p>The library uses an adapter pattern to support multiple AI providers without changing your application code.</p><h3 id="what-is-an-adapter" tabindex="-1">What is an Adapter <a class="header-anchor" href="#what-is-an-adapter" aria-label="Permalink to &quot;What is an Adapter&quot;">​</a></h3><p>An adapter is a function with this signature:</p><div class="language-tsx vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">tsx</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">type</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> LLMAdapter</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">  transcript</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">:</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> string</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">  commands</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">:</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> Array</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">&lt;{ </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">id</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">:</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> string</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">; </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">description</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">:</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> string</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">; </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">phrase</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">?:</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> string</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> }&gt;</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=&gt;</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> Promise</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">&lt;{ </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">commandId</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">:</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> string</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> |</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> null</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> }&gt;;</span></span></code></pre></div><p>Every adapter receives:</p><ul><li>The user&#39;s spoken transcript</li><li>A list of available commands (without the action functions for security)</li></ul><p>Every adapter returns:</p><ul><li>An object with the matching command ID or null</li></ul><h3 id="why-this-design" tabindex="-1">Why This Design <a class="header-anchor" href="#why-this-design" aria-label="Permalink to &quot;Why This Design&quot;">​</a></h3><p>This design has several benefits:</p><ol><li><strong>Provider Independence</strong> - Switch from OpenAI to Gemini by changing one line of code</li><li><strong>Security</strong> - Action functions are never sent to the AI</li><li><strong>Testability</strong> - Create mock adapters for testing without API calls</li><li><strong>Extensibility</strong> - Support any AI provider including local models</li></ol><h2 id="context-aware-commands" tabindex="-1">Context-Aware Commands <a class="header-anchor" href="#context-aware-commands" aria-label="Permalink to &quot;Context-Aware Commands&quot;">​</a></h2><p>One of the key features is that commands are context-aware. The AI only sees commands that are currently registered, which typically means commands from components currently visible on the screen.</p><h3 id="how-this-reduces-costs" tabindex="-1">How This Reduces Costs <a class="header-anchor" href="#how-this-reduces-costs" aria-label="Permalink to &quot;How This Reduces Costs&quot;">​</a></h3><p>Consider an application with 50 total voice commands spread across 10 pages. Without context awareness, every voice input would send all 50 commands to the AI, using more tokens and increasing costs.</p><p>With React Voice Action Router:</p><ul><li>Page A has 5 commands registered</li><li>User speaks on Page A</li><li>Only 5 commands are sent to the AI</li><li>Token usage is reduced by 90%</li></ul><h3 id="how-this-reduces-hallucinations" tabindex="-1">How This Reduces Hallucinations <a class="header-anchor" href="#how-this-reduces-hallucinations" aria-label="Permalink to &quot;How This Reduces Hallucinations&quot;">​</a></h3><p>AI models can sometimes match a transcript to an unrelated command. With fewer commands in the prompt:</p><ul><li>The AI has less choices to pick from</li><li>The AI is less likely to match to irrelevant commands</li><li>The user experience is more predictable</li></ul><h2 id="the-prompt-engineering" tabindex="-1">The Prompt Engineering <a class="header-anchor" href="#the-prompt-engineering" aria-label="Permalink to &quot;The Prompt Engineering&quot;">​</a></h2><p>Each adapter uses a carefully crafted prompt to guide the AI. Here is the core prompt structure:</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>You are a precise Voice Command Router.</span></span>
<span class="line"><span>Your goal is to map the user&#39;s spoken input to the correct Command ID.</span></span>
<span class="line"><span></span></span>
<span class="line"><span>RULES:</span></span>
<span class="line"><span>1. Analyze the user&#39;s input and find the intent.</span></span>
<span class="line"><span>2. Match it to the command with the most relevant description.</span></span>
<span class="line"><span>3. Use fuzzy matching (e.g., &quot;Dark mode&quot; matches &quot;Toggle Theme&quot;).</span></span>
<span class="line"><span>4. If NO command matches the intent, return null.</span></span>
<span class="line"><span>5. Output ONLY valid JSON.</span></span>
<span class="line"><span></span></span>
<span class="line"><span>AVAILABLE COMMANDS:</span></span>
<span class="line"><span>- ID: &quot;nav_home&quot; | Description: &quot;Navigate to the home page&quot;</span></span>
<span class="line"><span>- ID: &quot;toggle_theme&quot; | Description: &quot;Toggle between light and dark mode&quot;</span></span>
<span class="line"><span></span></span>
<span class="line"><span>RESPONSE FORMAT:</span></span>
<span class="line"><span>{ &quot;commandId&quot;: &quot;string_id_or_null&quot; }</span></span></code></pre></div><p>This prompt:</p><ul><li>Clearly defines the task</li><li>Sets expectations for fuzzy matching</li><li>Enforces JSON-only output</li><li>Lists available commands with descriptions</li></ul><h2 id="transfer-control-pausing" tabindex="-1">Transfer Control &amp; Pausing <a class="header-anchor" href="#transfer-control-pausing" aria-label="Permalink to &quot;Transfer Control &amp; Pausing&quot;">​</a></h2><p>In complex voice applications, there are times when you need to capture raw speech without triggering commands (e.g., dictating an email or searching for a song title).</p><p>The library provides a <strong>Gatekeeper Mechanism</strong> via the <code>setPaused()</code> control.</p><ol><li><p><strong>Paused State</strong>: When <code>isPaused</code> is set to <code>true</code>, the <code>processTranscript</code> function acts as a dead end. It logs the input but <strong>does not</strong> proceed to matching (neither Exact nor Fuzzy).</p></li><li><p><strong>Resumed State</strong>: When set back to <code>false</code>, normal routing resumes immediately.</p></li></ol><p>This allows you to &quot;borrow&quot; the microphone for other purposes without unmounting the provider.</p><h2 id="the-speech-engine-v2-0" tabindex="-1">The Speech Engine (v2.0) <a class="header-anchor" href="#the-speech-engine-v2-0" aria-label="Permalink to &quot;The Speech Engine (v2.0)&quot;">​</a></h2><p>In version 2.0, we introduced a robust internal Speech Engine that wraps the browser&#39;s <code>SpeechRecognition</code> API.</p><h3 id="auto-restart-logic" tabindex="-1">Auto-Restart Logic <a class="header-anchor" href="#auto-restart-logic" aria-label="Permalink to &quot;Auto-Restart Logic&quot;">​</a></h3><p>Browsers (especially Chrome) automatically stop the speech recognition service after a few seconds of silence or network inactivity. This is frustrating for &quot;always-on&quot; voice control.</p><p>Our engine monitors the <code>onend</code> event. If the <code>isListening</code> state is meant to be true, it immediately restarts the service. This creates the illusion of a continuous stream, handling the restart logic so you don&#39;t have to.</p><h3 id="stale-closure-prevention" tabindex="-1">Stale Closure Prevention <a class="header-anchor" href="#stale-closure-prevention" aria-label="Permalink to &quot;Stale Closure Prevention&quot;">​</a></h3><p>Since the Speech API runs outside of React&#39;s render cycle, event listeners can often capture &quot;stale&quot; React state. We use a <strong>Ref-based architecture</strong> internally to ensure that the microphone always has access to the latest application state (like <code>isPaused</code> or <code>isDictating</code>) without needing to detach and re-attach listeners.</p><h2 id="offline-fallback" tabindex="-1">Offline Fallback <a class="header-anchor" href="#offline-fallback" aria-label="Permalink to &quot;Offline Fallback&quot;">​</a></h2><p>Reliance on AI APIs introduces a single point of failure. If the user loses internet connection or the API is down, voice control usually breaks.</p><p>We implemented a <strong>Local Fallback Matcher</strong> that kicks in whenever the AI adapter throws an error.</p><ol><li><strong>Tokenization:</strong> It splits the user&#39;s transcript into keywords.</li><li><strong>Scoring:</strong> It checks these keywords against the <code>description</code> and <code>phrase</code> of registered commands.</li><li><strong>Threshold:</strong> If a command has a high enough overlap score, it is executed locally.</li></ol><p>This ensures that critical navigation and control commands (&quot;Go home&quot;, &quot;Stop&quot;, &quot;Dark mode&quot;) continue to work offline.</p><h2 id="performance-considerations" tabindex="-1">Performance Considerations <a class="header-anchor" href="#performance-considerations" aria-label="Permalink to &quot;Performance Considerations&quot;">​</a></h2><h3 id="exact-match-first" tabindex="-1">Exact Match First <a class="header-anchor" href="#exact-match-first" aria-label="Permalink to &quot;Exact Match First&quot;">​</a></h3><p>By checking exact matches before calling the AI, the library minimizes latency for known phrases. If users learn the exact phrases, they get sub-millisecond response times.</p><h3 id="minimal-payload" tabindex="-1">Minimal Payload <a class="header-anchor" href="#minimal-payload" aria-label="Permalink to &quot;Minimal Payload&quot;">​</a></h3><p>The library only sends essential data to the AI:</p><ul><li>The transcript</li><li>Command IDs and descriptions</li><li>Action functions are never sent</li></ul><p>This reduces payload size and protects your code.</p><h3 id="no-re-registration-on-render" tabindex="-1">No Re-Registration on Render <a class="header-anchor" href="#no-re-registration-on-render" aria-label="Permalink to &quot;No Re-Registration on Render&quot;">​</a></h3><p>Thanks to the proxy pattern, commands do not re-register on every render. The useVoiceCommand hook only registers once when the component mounts and unregisters once when it unmounts.</p><h2 id="headless-design-philosophy" tabindex="-1">Headless Design Philosophy <a class="header-anchor" href="#headless-design-philosophy" aria-label="Permalink to &quot;Headless Design Philosophy&quot;">​</a></h2><p>React Voice Action Router is intentionally headless, meaning it provides no UI components. This design choice gives you complete control over:</p><ul><li>How voice input is captured (Web Speech API, Deepgram, Whisper, etc.)</li><li>How to display processing states</li><li>Whether to show any UI at all (voice can be completely invisible)</li><li>The look and feel of any voice indicators</li></ul><p>The library handles the complex logic of command registration, routing, and AI matching. You handle everything else.</p>`,82)])])}const m=a(n,[["render",o]]);export{u as __pageData,m as default};
